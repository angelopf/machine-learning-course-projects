{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Ângelo Pascoal Figueiral, 44645</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 Machine Learning\n",
    "\n",
    "This assignment will contain 2 questions with details as below. The due date is Feburary 28 (Sunday), 2020 23:59PM. Each late day will result in 20% loss of total points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1  (20 points) Make a plan before running your model\n",
    "\n",
    "Joana Gonzales, a young professional looking to diversify her investment portfolio. Joana graduated with a Masters in Business Analytics, and after four successful years as a product manager in a tech company, she has managed to save a sizable amount of money. She now wants to start diversifying her savings portfolio. So far, she has focused on traditional investments (stocks, bonds, etc.) and she now wants to look further afield. One asset class she is particularly interested in is peer-to-peer loans issued on online platforms. The high returns advertised by these platforms seem to be an attractive value proposition, and Joana is especially excited by the large amount of data these platforms make publicly available. With her data science background, she is hoping to apply machine learning tools to these data to come up with lucrative investment strategies. \n",
    "\n",
    "Peer-to-peer lending refers to the practice of lending money to individuals (or small businesses) via online services that match anonymous lenders with borrowers. Lenders can typically earn higher returns relative to savings and investment products offered by banking institutions. However, there is of course the risk that the borrower defaults on his or her loan. Interest rates are usually set by an intermediary platform on the basis of analyzing the borrower's credit (using features such as FICO score, employment status, annual income, debt-to-income ratio, number of open credit lines). The intermediary platform generates revenue by collecting a one-time fee on funded loans (from borrowers) and by charging a loan servicing fee to investors.\n",
    "\n",
    "The peer-to-peer lending industry in the United States started in February 2006 with the launch of Prosper,‡ followed by LendingClub.§ In 2008, the Securities and Exchange Commission (SEC) required that peer-to-peer companies register their offerings as securities, pursuant to the Securities Act of 1933. Both Prosper and LendingClub gained approval from the SEC to offer investors notes backed by payments received on the loans. One of the interesting features of the peer-to-peer lending market is the richness of the historical data available. The two largest U.S. platforms (LendingClub and Prosper) have chosen to give free access to their data to potential investors. The definition of each loan status is as follows. Current refers to a loan that is still being reimbursed in a timely manner. Late corresponds to a loan on which a payment is between 16 and 120 days overdue. If the payment is delayed by more than 121 days, the loan is considered to be in Default. \n",
    "\n",
    "\n",
    "\n",
    "If you were Joana, your job is to define investment strategies. Given your knowledge of data science, below you need to write a plan (~500 words), using the steps specified from business problem to machine learning problem. Writing down in the below cell using Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "Before deciding to to use a Machine Learning model to help her decisions, Joana must evaluate if and how her strategy could be enhaced with such model. To assess this need, the first question is whether there's room for **task automation**. Since analyzing loan possibilities case by case would be a very lenghty and manual process, she definetely needs some sort of automatic classification method, one that would learn overtime. Another good indicator that machine learning is applicable here is the **high complexity of the rules** that set a loan's probability of success.\n",
    "\n",
    "The large quantity of historical data available, structured the right way, could be helpful on building a model that would **return Joana a list of the loan opportunities that are most likely to payoff**, considering both the potential returns and default probability. \n",
    "\n",
    "When it comes to data, Joana needs to ensure its **quality** and **quantity**: is there enough historical labeled data to build a good and efficient model? Is it going to be helpful to reach the desired output? She might use interest rates, loan amounts, monthly instalments and attributes related to the borrower (FICO score, annual income, etc) to build this model. Since the model would derive its parameters from data, Joana needs to analyze if there are **logical patterns in the relation betweem the independent variables and the returns** (by the nature of the task, one would expect there is). If it is patterned data then it is possible to extract output from it, otherwise the bad data would make the use of machine learning inefficient.\n",
    "\n",
    "Once the data has been structured and assuming there is enough labeled data and there are meaningful patterns, she can define the classes for the algorithm's output, which would **represent the outputs in a rank of loan opportunities** (A-F, 0-10, etc).\n",
    "\n",
    "Crucially, Joana needs to optimize this model by training the model, minimizing the error. A common used approach in portfolio optimization is to consider the standard deviation/variance of the return, that is, to account for the portfolio risk. In this case, k-means could be used to cluster loan opportunities.\n",
    "\n",
    "At last, in order to keep the model efficient, it needs to be constantly fed with labels to its ongoing predictions, creating a learning cycle, since the nature of the data keeps changing. That is, Joana would feed the algorithm with the results of her investments, so that the model could update its parameters based on the new data. Ideally, this process would be automatic.\n",
    "\n",
    "The definition of an **appropriate metric for success** would be crucial, and in this case it could be the percentage of loans where the model correctly predicted to profit, or, in a more complex level, the level of returns predicted versus the level of returns obtained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (50 points) Zestimate this house\n",
    "\n",
    "Purchasing a house is a very big decision for most of us. Companies such as Zillows collected tons of data regarding the listing and sold price of American houses and build the predictive model, named *Zestimate*. You are expected to build a model similar as Zestimate to predict house price in Boston. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 (10 points) \n",
    "Create train and test set, each contains 80% and 20% of the dataset, respectively using *train_test_split* function in scikit-learn. Train a linear model on the train set and test on the test set, report the training error and test errors, respectively (as mean squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set MSE: 19.33\n",
      "Test set MSE: 33.45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=0)\n",
    "\n",
    "# creating the linear regression model\n",
    "lr = LinearRegression(normalize=True)\n",
    "\n",
    "# fitting the linear regression model to the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# estimating y with the model\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "# calculating training and test mean squared errors\n",
    "mse_train = mean_squared_error(y_train, y_hat_train)\n",
    "mse_test = mean_squared_error(y_test, y_hat_test)\n",
    "\n",
    "\n",
    "print(f\"\"\"Training set MSE: {round(mse_train,2)}\\nTest set MSE: {round(mse_test, 2)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2 (10 points)\n",
    "\n",
    "Perform a 10-fold cross-validation on the whole data set. Show the averaged mean sqaured error on both train and test set at each fold, explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train set MSE</th>\n",
       "      <th>Test set MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.33</td>\n",
       "      <td>33.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.76</td>\n",
       "      <td>35.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.25</td>\n",
       "      <td>29.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.80</td>\n",
       "      <td>23.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.06</td>\n",
       "      <td>18.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.76</td>\n",
       "      <td>15.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21.99</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.58</td>\n",
       "      <td>19.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21.01</td>\n",
       "      <td>26.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.02</td>\n",
       "      <td>30.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>21.26</td>\n",
       "      <td>25.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train set MSE  Test set MSE\n",
       "1                19.33         33.45\n",
       "2                18.76         35.23\n",
       "3                20.25         29.27\n",
       "4                21.80         23.68\n",
       "5                23.06         18.31\n",
       "6                23.76         15.51\n",
       "7                21.99         24.49\n",
       "8                22.58         19.75\n",
       "9                21.01         26.98\n",
       "10               20.02         30.96\n",
       "Average          21.26         25.76"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# performing 2-degree polynomial regression with cross validation while keeping the train and test errors at each fold\n",
    "scores = cross_validate(lr, X, y, scoring='neg_mean_squared_error', cv=cv_custom_split, return_train_score=True)\n",
    "\n",
    "# creating a dataframe for proper error visualization (each fold and average)\n",
    "    # each fold\n",
    "dic_scores = {'Train set MSE': -np.round(scores['train_score'], 2),\n",
    "        'Test set MSE': -np.round(scores['test_score'], 2)\n",
    "       }\n",
    "\n",
    "df_scores = pd.DataFrame(dic_scores, index = [i for i in range(1, 11)])\n",
    "\n",
    "    # average \n",
    "avg_train_mse = round(np.mean(df_scores['Train set MSE']), 2)\n",
    "avg_test_mse = round(np.mean(df_scores['Test set MSE']), 2)\n",
    "\n",
    "avg_df = pd.DataFrame([[avg_train_mse, avg_test_mse]], columns=['Train set MSE', 'Test set MSE'], index=['Average'])\n",
    "df_scores.append(avg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings (Question 2.2):\n",
    "\n",
    "Cross-validation helps us understand that the test error previously obtained (33.45) is not even close to the average error when splitting the dataset randomly. With these 10 folds, we realise the test error is much lower than we would think had we not used cross-validation. Additionally, it allows us to verify that the test error has quite some variance, since values have a large range (about 20 units), while the train error does not, since values have a narrow range (about 4 units).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3 (10 points) \n",
    " \n",
    "Add 2-degree polynomial features (with no interactions) and perform 10-fold cross-validation on the whole data set. Show the mean sqaured error on both train and test set at each fold, explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train set MSE</th>\n",
       "      <th>Test set MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.93</td>\n",
       "      <td>25.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.91</td>\n",
       "      <td>30.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.21</td>\n",
       "      <td>19.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.84</td>\n",
       "      <td>13.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.75</td>\n",
       "      <td>13.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.27</td>\n",
       "      <td>11.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.47</td>\n",
       "      <td>20.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.54</td>\n",
       "      <td>14.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.94</td>\n",
       "      <td>17.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.56</td>\n",
       "      <td>27.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>13.44</td>\n",
       "      <td>19.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train set MSE  Test set MSE\n",
       "1                11.93         25.65\n",
       "2                10.91         30.49\n",
       "3                13.21         19.68\n",
       "4                14.84         13.01\n",
       "5                14.75         13.06\n",
       "6                15.27         11.56\n",
       "7                13.47         20.11\n",
       "8                14.54         14.51\n",
       "9                13.94         17.17\n",
       "10               11.56         27.78\n",
       "Average          13.44         19.30"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding polynomial terms WITHOUT INTERACTIONS\n",
    "X_poly_no_interactions = np.hstack((X, X**2))\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# performing linear regression of the 2-degree polynomials with cross validation while keeping the train and test errors at each fold\n",
    "scores = cross_validate(lr, X_poly_no_interactions, y, scoring='neg_mean_squared_error', cv=cv_custom_split, return_train_score=True)\n",
    "\n",
    "# creating a dataframe for proper error visualization (each fold and average)\n",
    "    # each fold\n",
    "dic_scores = {'Train set MSE': -np.round(scores['train_score'], 2),\n",
    "        'Test set MSE': -np.round(scores['test_score'], 2)\n",
    "       }\n",
    "\n",
    "df_scores = pd.DataFrame(dic_scores, index = [i for i in range(1, 11)])\n",
    "\n",
    "    # average \n",
    "avg_train_mse = round(np.mean(df_scores['Train set MSE']), 2)\n",
    "avg_test_mse = round(np.mean(df_scores['Test set MSE']), 2)\n",
    "\n",
    "avg_df = pd.DataFrame([[avg_train_mse, avg_test_mse]], columns=['Train set MSE', 'Test set MSE'], index=['Average'])\n",
    "df_scores.append(avg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT CELLS BELLOW AND RUN TO INCLUDE INTERACTION TERMS\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# # making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "# cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# # performing linear regression of the 2-degree polynomials with cross validation while keeping the train and test errors at each fold\n",
    "# scores = cross_validate(lr, X_poly, y, scoring='neg_mean_squared_error', cv=cv_custom_split, return_train_score=True)\n",
    "\n",
    "# # creating a dataframe for proper error visualization (each fold and average)\n",
    "#     # each fold\n",
    "# dic_scores = {'Train set MSE': -np.round(scores['train_score'], 2),\n",
    "#         'Test set MSE': -np.round(scores['test_score'], 2)\n",
    "#        }\n",
    "\n",
    "# df_scores = pd.DataFrame(dic_scores, index = [i for i in range(1, 11)])\n",
    "\n",
    "#     # average \n",
    "# avg_train_mse = round(np.mean(df_scores['Train set MSE']), 2)\n",
    "# avg_test_mse = round(np.mean(df_scores['Test set MSE']), 2)\n",
    "\n",
    "# avg_df = pd.DataFrame([[avg_train_mse, avg_test_mse]], columns=['Train set MSE', 'Test set MSE'], index=['Average'])\n",
    "# df_scores.append(avg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings (Question 2.3):\n",
    "\n",
    "- As expected, inclduing polynomial transformations of the features improved the fit to the training data, which is why the training error decreased about 35% on avg\n",
    "- The avg decrease of the test error (about 25%) shows that the model improved, and that two degree polynomials are within the range of no overfitting\n",
    "- Although the test error in some folds is still quite high, it's worth noting that it decreased in each and every fold (comparing with the linear combination of features)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.4 (20 points)\n",
    "\n",
    "Perform cross-validation using ridge regression and lasso regression on different feature combinations (linear features vs. polynomial features respectively. Explain which method works better in this case. Check the coefficient and explain the differences between ridge regression and lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: \n",
      "Coefficients:\t[-1.16807614e-01  4.60034842e-02 -2.37620690e-02  2.27814972e+00\n",
      " -8.55779612e+00  3.75513528e+00 -1.04143035e-02 -1.28009479e+00\n",
      "  2.22037885e-01 -1.15255734e-02 -9.69288272e-01  8.53481709e-03\n",
      " -4.98849035e-01]\n",
      "Test MSE:\t[34.23160611]\n",
      "\n",
      "Fold 2: \n",
      "Coefficients:\t[-9.02872707e-02  3.11974576e-02 -2.28383681e-02  2.16297753e+00\n",
      " -9.16448771e+00  4.12731175e+00 -2.80443971e-03 -1.17165539e+00\n",
      "  2.56094400e-01 -1.28543938e-02 -8.75008618e-01  8.24207592e-03\n",
      " -5.27186101e-01]\n",
      "Test MSE:\t[35.77716764]\n",
      "\n",
      "Fold 3: \n",
      "Coefficients:\t[-9.14358739e-02  4.60844054e-02 -6.77035843e-03  1.36167312e+00\n",
      " -9.38886277e+00  3.94430321e+00 -2.79920771e-03 -1.28408137e+00\n",
      "  3.03676077e-01 -1.41502228e-02 -9.57329349e-01  7.92132130e-03\n",
      " -5.39016424e-01]\n",
      "Test MSE:\t[29.56036621]\n",
      "\n",
      "Fold 4: \n",
      "Coefficients:\t[ -0.12968252   0.05103616  -0.04858725   2.35325635 -10.18331896\n",
      "   3.59761433  -0.01184109  -1.34775461   0.32059548  -0.01253183\n",
      "  -0.749863     0.01074167  -0.50003508]\n",
      "Test MSE:\t[23.75893017]\n",
      "\n",
      "Fold 5: \n",
      "Coefficients:\t[-1.07404621e-01  5.13583182e-02 -6.61510835e-03  3.77086643e+00\n",
      " -1.03816096e+01  3.52721546e+00 -8.81189454e-03 -1.43713142e+00\n",
      "  3.13379845e-01 -1.32282264e-02 -8.30168927e-01  1.08270068e-02\n",
      " -5.42025798e-01]\n",
      "Test MSE:\t[18.12860286]\n",
      "\n",
      "Fold 6: \n",
      "Coefficients:\t[-1.17219237e-01  5.81887935e-02 -7.01369698e-03  3.28584541e+00\n",
      " -1.02264918e+01  3.47869096e+00 -1.33177726e-02 -1.60276492e+00\n",
      "  3.03994456e-01 -1.26847774e-02 -8.93045146e-01  9.46534273e-03\n",
      " -5.46198468e-01]\n",
      "Test MSE:\t[15.50196801]\n",
      "\n",
      "Fold 7: \n",
      "Coefficients:\t[ -0.09796247   0.05051738   0.01647551   4.14221696 -10.0712316\n",
      "   3.77127857   0.01453302  -1.21063039   0.31891617  -0.0123619\n",
      "  -0.84543355   0.0102792   -0.64087119]\n",
      "Test MSE:\t[24.6158093]\n",
      "\n",
      "Fold 8: \n",
      "Coefficients:\t[-8.69274659e-02  4.80678176e-02  2.64305780e-02  2.86088596e+00\n",
      " -9.76863740e+00  3.99648648e+00 -8.87754942e-03 -1.33831631e+00\n",
      "  2.69647552e-01 -1.28818390e-02 -8.73075238e-01  1.02277998e-02\n",
      " -5.38871944e-01]\n",
      "Test MSE:\t[19.93546242]\n",
      "\n",
      "Fold 9: \n",
      "Coefficients:\t[-9.79364987e-02  4.61988470e-02 -6.82828644e-02  1.11336766e+00\n",
      " -1.03829242e+01  3.48352520e+00  7.98137205e-03 -1.38285377e+00\n",
      "  2.78031532e-01 -1.29449533e-02 -8.72506323e-01  7.19933367e-03\n",
      " -5.97285462e-01]\n",
      "Test MSE:\t[27.10873271]\n",
      "\n",
      "Fold 10: \n",
      "Coefficients:\t[-1.25522117e-01  3.98753052e-02 -2.20392032e-02  2.19705788e+00\n",
      " -8.90110382e+00  4.48061282e+00 -1.85490584e-02 -1.50436129e+00\n",
      "  2.52575945e-01 -1.30905399e-02 -9.04066672e-01  7.52414326e-03\n",
      " -4.70625044e-01]\n",
      "Test MSE:\t[31.38130457]\n",
      "\n",
      "\n",
      "AVERAGE TEST MSE: 26.0\n"
     ]
    }
   ],
   "source": [
    "# Ridge, Linear features\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=0)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# performing linear regression with cross validation\n",
    "scores_ridge_linear = cross_validate(ridge_reg, X, y, scoring='neg_mean_squared_error', cv=cv_custom_split, return_train_score=True, return_estimator=True)\n",
    "\n",
    "# displaying\n",
    "for (i, fold) in zip(range(1, len(scores_ridge_linear['estimator'])+1), scores_ridge_linear['estimator']):\n",
    "    print(f\"Fold {i}: \\nCoefficients:\\t{fold.coef_}\\nTest MSE:\\t{np.abs([scores_ridge_linear['test_score'][i-1]])}\\n\")  # coefficients and test error for each fold\n",
    "\n",
    "print(f\"\\nAVERAGE TEST MSE: {round(np.mean(-scores_ridge_linear['test_score']), 2)}\")  # average test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: \n",
      "Coefficients:\t[-0.05889028  0.05317657 -0.          0.         -0.          0.67954962\n",
      "  0.01684077 -0.6487664   0.198738   -0.01399421 -0.86421958  0.00660309\n",
      " -0.73120957]\n",
      "Test MSE:\t[41.7000968]\n",
      "\n",
      "Fold 2: \n",
      "Coefficients:\t[-0.04944984  0.0302011  -0.00328382  0.         -0.          1.07546833\n",
      "  0.02734036 -0.45342361  0.23525599 -0.01550802 -0.70649063  0.00644508\n",
      " -0.75565653]\n",
      "Test MSE:\t[42.37487215]\n",
      "\n",
      "Fold 3: \n",
      "Coefficients:\t[-0.05218059  0.05322569 -0.          0.         -0.          1.01086155\n",
      "  0.02760415 -0.60592167  0.28355942 -0.0159251  -0.82848051  0.00643565\n",
      " -0.7742264 ]\n",
      "Test MSE:\t[34.56153127]\n",
      "\n",
      "Fold 4: \n",
      "Coefficients:\t[-0.09285087  0.05307795 -0.04850117  0.         -0.          0.30312977\n",
      "  0.00942084 -0.67086372  0.30286018 -0.01472956 -0.53572375  0.00843276\n",
      " -0.73177378]\n",
      "Test MSE:\t[36.04668483]\n",
      "\n",
      "Fold 5: \n",
      "Coefficients:\t[-0.05686244  0.04757585 -0.          0.         -0.          0.8900795\n",
      "  0.01813494 -0.70991685  0.27309218 -0.01491353 -0.70816862  0.00954916\n",
      " -0.76280923]\n",
      "Test MSE:\t[21.22876714]\n",
      "\n",
      "Fold 6: \n",
      "Coefficients:\t[-0.07049061  0.06050536 -0.          0.         -0.          0.68418621\n",
      "  0.01814427 -0.89326236  0.27466467 -0.01500466 -0.78721591  0.00769657\n",
      " -0.77285597]\n",
      "Test MSE:\t[18.68717314]\n",
      "\n",
      "Fold 7: \n",
      "Coefficients:\t[-0.05877939  0.05114549 -0.          0.         -0.          0.93124428\n",
      "  0.04364451 -0.55851066  0.30599619 -0.01482576 -0.71852278  0.00910017\n",
      " -0.85420378]\n",
      "Test MSE:\t[25.73765388]\n",
      "\n",
      "Fold 8: \n",
      "Coefficients:\t[-0.04195565  0.05260124 -0.          0.         -0.          0.92706684\n",
      "  0.02155389 -0.75293531  0.23599405 -0.01473803 -0.72635748  0.0079455\n",
      " -0.78807479]\n",
      "Test MSE:\t[23.67062166]\n",
      "\n",
      "Fold 9: \n",
      "Coefficients:\t[-0.05964829  0.05006839 -0.06572462  0.         -0.          0.71162577\n",
      "  0.02970247 -0.73478018  0.23883529 -0.01449206 -0.65242816  0.00579257\n",
      " -0.80612542]\n",
      "Test MSE:\t[34.20958716]\n",
      "\n",
      "Fold 10: \n",
      "Coefficients:\t[-0.08146348  0.04604629 -0.          0.         -0.          1.63134544\n",
      "  0.00842909 -0.8753279   0.22858758 -0.01601123 -0.74556415  0.00642049\n",
      " -0.68871291]\n",
      "Test MSE:\t[29.53151733]\n",
      "\n",
      "\n",
      "AVERAGE TEST MSE: 30.77\n"
     ]
    }
   ],
   "source": [
    "# Lasso, Linear features\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=1)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# performing linear regression with cross validation\n",
    "scores_lasso_linear = cross_validate(lasso_reg, X, y, scoring='neg_mean_squared_error', cv=cv_custom_split, return_train_score=True, return_estimator=True)\n",
    "\n",
    "# displaying\n",
    "for (i, fold) in zip(range(1, len(scores_lasso_linear['estimator'])+1), scores_lasso_linear['estimator']):\n",
    "    print(f\"Fold {i}: \\nCoefficients:\\t{fold.coef_}\\nTest MSE:\\t{np.abs([scores_lasso_linear['test_score'][i-1]])}\\n\")  # coefficients and test error for each fold\n",
    "\n",
    "print(f\"\\nAVERAGE TEST MSE: {round(np.mean(-scores_lasso_linear['test_score']), 2)}\")  # average test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-degree Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: \n",
      "Coefficients:\t[-4.32102027e-01 -2.74772129e-01 -3.01469114e+00 -2.66752762e-01\n",
      "  7.07210883e-02  2.53450048e+00  9.69023352e-01 -1.32361532e+00\n",
      "  1.15663777e+00 -2.78063460e-02 -8.15089189e-01 -5.93238902e-02\n",
      " -1.17432031e+00  2.04529959e-03  1.38354032e-01  5.34913523e-01\n",
      "  1.70117147e+00 -6.37177687e-01  3.25332107e-02 -4.92724926e-03\n",
      "  4.67210869e-02  5.11154837e-01 -3.87897771e-02  2.22081415e-01\n",
      "  2.64604226e-05  7.53656773e-03 -5.05548109e-04 -2.40719717e-03\n",
      " -5.56781798e-02 -5.37073543e-01  1.63173384e-02 -1.80529442e-04\n",
      " -1.65060835e-03 -4.50022838e-03  3.73548802e-04 -6.12942104e-03\n",
      "  1.25886222e-03 -4.45145033e-03  4.23666242e-02  1.29066824e-01\n",
      "  3.28737915e-01  2.31321275e-01  7.26530364e-03  1.32573081e-01\n",
      " -4.83770683e-02  1.69388957e-03 -7.94000915e-02  2.15872318e-03\n",
      " -2.83318859e-02 -2.66752762e-01 -1.73974383e+00 -5.56771079e+00\n",
      "  8.17496195e-02  9.97851622e-01  6.67576082e-01 -5.27449751e-02\n",
      "  5.93687999e-02  1.20160499e-01 -6.33104732e-01  1.86442189e-02\n",
      "  3.46375588e+00 -2.37062447e-01  2.36701974e+00 -1.33068356e+00\n",
      "  1.17094062e-02 -1.44478854e+00  6.11526940e-03  1.03307350e+00\n",
      "  1.02845392e+00 -7.57932450e-02  1.73941103e-01 -7.49005197e-02\n",
      " -1.13102963e-02 -4.33374966e-01  7.78867976e-03  2.44815266e-04\n",
      "  4.57591038e-04  1.11358370e-02  1.45802850e-02 -5.56718144e-04\n",
      " -9.96254732e-03 -5.91434497e-04 -8.14527882e-03  2.83589979e-01\n",
      " -6.64227245e-02 -1.30185554e-03 -1.22178115e-01 -8.98270226e-03\n",
      "  4.57799461e-02 -1.83952227e-01  1.12784939e-02 -2.88781254e-02\n",
      " -1.94278061e-03 -3.67382630e-02 -7.20046184e-05  5.41844794e-03\n",
      " -9.32748229e-06  5.13829899e-04  5.54219175e-02  5.13021558e-03\n",
      "  2.26113501e-02 -1.12228104e-05 -8.99342789e-05  2.60214571e-02]\n",
      "Test MSE:\t[31.72536988]\n",
      "\n",
      "Fold 2: \n",
      "Coefficients:\t[-9.19502109e-01 -3.16814887e-01 -2.15078895e+00 -2.26356258e-02\n",
      "  7.13625362e-02  2.27357978e+00  7.38150532e-01 -1.67371137e+00\n",
      "  1.11087269e+00 -1.50725092e-01 -9.27539607e-01  3.21127405e-02\n",
      "  1.30698868e+00  2.01727395e-03  1.02678954e-01  3.22049492e-01\n",
      "  6.58688687e-01 -1.38138972e+00  5.60560672e-02 -1.31114008e-03\n",
      "  2.27494493e-02  1.89474176e-01 -1.75909742e-02  1.19570855e-01\n",
      " -1.79925597e-04  1.35786220e-02 -2.63009489e-04 -7.02849593e-03\n",
      " -4.98279797e-02  2.26635425e-01  8.09862954e-04 -9.31957009e-04\n",
      " -1.24881796e-02 -7.71560758e-04  4.60407038e-04 -1.88767717e-04\n",
      "  5.95577960e-04 -3.29838841e-04  2.48114765e-02 -3.31426353e-01\n",
      "  7.34736291e-01  7.93501590e-02  4.68946603e-03  8.71491461e-02\n",
      "  4.48352904e-02  3.13469395e-04 -4.44333655e-02  2.22096853e-03\n",
      " -2.03491585e-02 -2.26356258e-02 -1.22894982e+00 -9.06327233e-01\n",
      " -8.11516515e-03  5.23546804e-01 -1.59108766e-01  1.46137358e-02\n",
      " -2.73828797e-02  1.69439277e-02  7.61486185e-02  1.28855918e-02\n",
      "  2.65925858e+00 -4.04324559e-01  1.68994242e+00 -8.02074011e-01\n",
      "  2.02607443e-02 -9.53978831e-01 -1.04393571e-02  9.14248997e-01\n",
      "  1.26881290e+00 -5.43304302e-02 -1.51722524e-01 -1.59139880e-01\n",
      " -1.79742525e-04 -2.98556781e-01 -3.47415233e-03 -2.44070015e-01\n",
      "  9.38538800e-04  1.41404519e-02  7.50151033e-03 -3.21495167e-04\n",
      " -5.10256433e-03 -6.11247257e-04 -7.03231537e-03  3.67650171e-01\n",
      " -1.04267850e-01 -2.78837390e-03 -1.30097165e-01 -1.67692060e-03\n",
      "  3.26868789e-02 -1.87218366e-01  7.65492472e-03  4.39915159e-02\n",
      " -1.79011806e-03 -3.30076127e-02 -1.67273174e-06  5.04516325e-03\n",
      "  5.79774797e-05  1.21376900e-04  3.20860159e-02  3.05617374e-03\n",
      "  8.92878723e-03 -4.87114904e-05 -7.89667763e-04  7.63490498e-03]\n",
      "Test MSE:\t[20.40582344]\n",
      "\n",
      "Fold 3: \n",
      "Coefficients:\t[-1.15544985e-01 -2.87487926e-02 -1.86840835e+00  6.99387946e-01\n",
      "  1.15853002e-01  3.57206944e+00  6.87022006e-01 -9.01950603e-01\n",
      "  1.75580967e+00 -7.29229773e-02 -1.46098652e+00  1.38624181e-01\n",
      "  1.05909530e+00  1.97570975e-03 -2.80136975e-02  2.05500453e-01\n",
      "  5.85417756e+00 -1.30054598e+00  8.42663846e-02  6.48119347e-04\n",
      " -1.62555648e-02  1.63176103e-01 -1.61969686e-02  1.48798634e-01\n",
      " -1.53392830e-04  1.61218315e-02 -4.52788368e-04 -2.35587462e-03\n",
      " -7.03970976e-02 -1.56169164e-01  3.34738318e-02 -9.27045015e-04\n",
      " -2.28779009e-03  4.21229870e-03  2.93164629e-04 -2.04398664e-03\n",
      " -1.73742692e-04 -5.02534725e-03  3.53221588e-02 -5.61059733e-01\n",
      "  5.65104828e-01  1.66240755e-01  3.60569577e-03  8.37149905e-02\n",
      "  1.51995597e-02  7.69390796e-04 -5.86838873e-02  4.19184109e-04\n",
      " -1.71839243e-02  6.99387946e-01 -3.50481860e-01 -3.72217781e+00\n",
      "  3.39912578e-02  5.99834296e-01 -1.17363851e+00 -2.06611602e-03\n",
      "  4.14052023e-01  6.18391533e-02 -1.21351695e-01  3.16784021e-02\n",
      "  2.67701442e+00 -3.06197959e-01  3.15404455e+00 -9.10106683e-01\n",
      "  1.24103486e-02 -8.32164664e-01 -9.46985619e-03  5.39339452e-01\n",
      "  8.40918101e-01 -4.64637100e-02 -1.67541011e-01 -3.06817943e-01\n",
      " -1.39522640e-03 -1.23710924e-01 -1.61542857e-03 -1.96217654e-01\n",
      "  6.79661863e-04  1.35771041e-02  1.24494874e-02 -4.99993497e-04\n",
      " -6.80113803e-03 -5.03395028e-04 -6.71710250e-03  3.28337438e-01\n",
      " -8.85109291e-02 -3.35586467e-03 -1.08467860e-01 -5.10445456e-03\n",
      "  3.18569218e-02 -1.58361310e-01  6.95932323e-03  4.78408940e-03\n",
      "  8.13622581e-04 -3.79044810e-02 -2.56848417e-05  6.04158195e-03\n",
      " -8.65039524e-05  2.92315714e-04  4.57252225e-02 -1.67722111e-04\n",
      "  7.70783084e-03 -4.76323416e-05 -4.24240518e-04  9.13477314e-03]\n",
      "Test MSE:\t[15.67421033]\n",
      "\n",
      "Fold 4: \n",
      "Coefficients:\t[-9.69754187e-01 -4.44551873e-01 -1.30845916e+00  8.42883055e-01\n",
      "  2.84787787e-02  3.31394113e+00  8.16978547e-01 -5.69148430e-01\n",
      "  1.59218581e+00  4.44359570e-04 -2.39902604e+00  2.79951413e-02\n",
      " -9.20427564e-01  1.23186284e-03  1.75315802e-01  4.31205535e-01\n",
      "  4.31093143e+00 -1.05701374e+00  8.30379247e-02 -8.43038737e-03\n",
      " -1.87727542e-01  4.63412443e-01 -3.75813784e-02  3.85109921e-01\n",
      " -1.79658245e-04  1.81533691e-02  1.01551846e-04 -1.25222997e-03\n",
      " -4.31994473e-02 -2.05835437e-01  1.00005300e-02  5.55496776e-04\n",
      " -8.42115063e-03 -1.02558216e-02  4.15518572e-04 -3.36725971e-03\n",
      "  1.29067796e-03 -6.05157598e-03  3.27793317e-02 -2.39608301e-02\n",
      "  1.28582006e+00 -7.95601920e-02  3.66001737e-03  5.54362638e-02\n",
      " -3.16684007e-02  1.29977996e-03 -8.81304840e-02  3.96709692e-03\n",
      " -2.62630343e-02  8.42883055e-01 -5.58428302e-01 -3.63014459e+00\n",
      "  5.50998092e-02  2.80209602e+00 -1.09719347e-01 -4.02475733e-02\n",
      "  8.53630699e-01  1.78531043e-02 -1.75291016e-01 -7.07096976e-02\n",
      "  2.09513847e+00 -3.61956908e-01  3.16542171e+00 -3.31106385e-01\n",
      " -4.85170385e-02 -1.03072928e+00  1.67970786e-02  1.29225043e+00\n",
      "  1.00113935e+00 -4.73665611e-02  8.93334987e-02 -1.79524884e-01\n",
      " -2.25256648e-03 -2.75696502e-01 -2.65305884e-03 -5.24903697e-03\n",
      "  4.10873237e-04 -2.39242809e-04  1.07994335e-02 -3.31708485e-04\n",
      " -2.04259414e-03 -8.22419383e-04 -7.28459210e-03  2.71334289e-01\n",
      " -7.66173168e-02 -3.70215250e-03 -5.02515244e-02 -9.75981333e-03\n",
      "  9.21085936e-02 -1.48279655e-01  8.19182102e-03 -5.87569455e-02\n",
      "  1.61156932e-03 -3.70211089e-02 -6.80957201e-05  8.39942379e-03\n",
      " -2.55981572e-04  3.22949743e-05 -1.39530644e-02  8.39151829e-03\n",
      "  1.48136889e-02 -4.26294362e-05 -6.85533390e-04  2.16261976e-02]\n",
      "Test MSE:\t[15.95018652]\n",
      "\n",
      "Fold 5: \n",
      "Coefficients:\t[-9.14363306e-01 -5.59254548e-01 -2.31594302e+00  1.24539428e+00\n",
      "  3.24501445e-02  4.22380700e+00  9.80862379e-01 -3.68848933e-01\n",
      "  1.31455029e+00 -2.33093788e-02 -2.30337810e+00  1.21917074e-01\n",
      " -3.12858305e-01  4.59185255e-03  1.39230528e-01  1.60361969e-01\n",
      "  3.52731558e+00 -8.92518015e-01  1.64202682e-01 -6.07566168e-03\n",
      " -1.26572360e-02  8.14315494e-02 -9.78744815e-03  9.98590803e-02\n",
      " -3.09701319e-05  1.65179751e-02  5.80119256e-04 -1.96175367e-03\n",
      "  2.63581991e-02  3.53773289e-01  1.08029691e-02 -1.24055053e-03\n",
      " -1.35045559e-02 -4.24579419e-03  3.53933581e-04  5.96905921e-03\n",
      "  5.38968129e-04  1.52975695e-03  2.34760084e-02 -9.45531824e-02\n",
      "  1.59259740e+00  9.22725637e-02  5.41875831e-03  8.18937313e-02\n",
      "  1.13477632e-02  6.24577969e-04 -3.52773276e-02  1.22592593e-03\n",
      " -2.74358603e-02  1.24539428e+00 -6.36073804e-01 -4.09889821e+00\n",
      "  1.16754136e-01  6.31492931e-01 -5.90070010e-01 -2.74454847e-02\n",
      "  9.22778361e-01  3.86537671e-02 -4.59715142e-01 -1.51148127e-01\n",
      "  3.92720414e+00 -5.26540365e-01  2.59328333e+00 -8.89781909e-01\n",
      " -1.18982226e-02 -1.07980042e+00 -3.63794725e-02  1.68593312e+00\n",
      "  7.26546007e-01 -5.86018016e-02  1.86537867e-02 -2.08205441e-01\n",
      " -5.33903494e-03 -1.56665992e-01  1.97918198e-03 -1.58419751e-01\n",
      "  4.87163878e-04  1.02458297e-02  1.03289627e-02 -3.35641056e-04\n",
      " -8.10492053e-03 -7.02972622e-04 -7.80669672e-03  3.51787569e-01\n",
      " -1.43902148e-01 -2.63428922e-03 -1.57572933e-01 -6.03937229e-03\n",
      "  3.23730648e-02 -1.48385758e-01  5.85434483e-03  2.46164407e-02\n",
      "  1.24801508e-03 -2.91466571e-02 -3.49475540e-06  5.40139847e-03\n",
      " -1.25244728e-04 -6.41806180e-04  6.02674078e-02  1.02116264e-03\n",
      "  4.07016102e-02 -2.88277979e-05 -3.49122982e-04  1.84553002e-02]\n",
      "Test MSE:\t[13.27821536]\n",
      "\n",
      "Fold 6: \n",
      "Coefficients:\t[-5.47682256e-01 -1.63663875e-01 -1.53274747e+00  5.49047557e-01\n",
      "  1.13421884e-01  4.12582870e+00  8.71014388e-01 -7.16775368e-01\n",
      "  1.63544856e+00 -6.37685447e-02 -1.39771394e+00  1.50564974e-01\n",
      " -2.07418496e-02  7.76307610e-04  2.58495147e-01  2.26140814e-01\n",
      "  3.18311523e+00 -1.40897643e+00  9.73461674e-02 -5.41500384e-03\n",
      " -1.44366179e-01  3.62603462e-02 -7.91268425e-03  7.00018854e-02\n",
      " -1.43720446e-04  1.69166310e-02  1.25270535e-04 -2.47878875e-03\n",
      " -9.77043169e-02 -1.02985899e-02  9.24629529e-03 -2.13350690e-04\n",
      " -4.32314756e-03 -4.29689825e-03  3.49677638e-04 -2.63176028e-03\n",
      "  3.21691629e-04 -5.67193543e-03  2.61955715e-02 -9.05990832e-01\n",
      "  1.29299847e+00  1.72200040e-01  5.80588204e-03  9.74462742e-02\n",
      "  5.40750815e-03 -8.70293421e-05 -6.41972544e-02 -2.25349053e-04\n",
      " -2.18641263e-02  5.49047557e-01 -1.29216767e-01 -5.81526371e+00\n",
      "  9.82777943e-02  3.03571266e+00 -1.77703475e+00  6.88948484e-02\n",
      "  7.82423173e-01  2.75010313e-02 -6.20015954e-01 -1.45920572e-03\n",
      "  3.38087948e+00 -4.60658880e-01  3.23950174e+00 -1.34292586e+00\n",
      "  1.77001177e-02 -1.38672276e+00 -2.70284637e-02  1.44662595e+00\n",
      "  8.73048995e-01 -4.42664684e-02  2.59053919e-01 -1.16252603e-01\n",
      " -1.09126699e-02 -1.94538688e-01 -1.67589231e-03 -1.77571424e-01\n",
      "  1.01004789e-04 -6.54625868e-05  1.36654532e-02 -3.99837907e-04\n",
      " -7.82582234e-03 -6.03656252e-04 -6.13470089e-03  2.53231864e-01\n",
      " -7.12216726e-02 -4.67633626e-03 -1.08310678e-01 -9.06519001e-03\n",
      "  8.33590770e-02 -1.03350047e-01  4.43769478e-03  4.56120995e-03\n",
      " -1.07023997e-03 -3.59292199e-02 -7.66824043e-06  7.64657098e-03\n",
      "  4.95056745e-06 -4.93640917e-04  3.58110543e-02  1.11960913e-03\n",
      "  3.96293594e-02 -5.29297222e-05 -8.75654242e-04  1.35755040e-02]\n",
      "Test MSE:\t[10.51664759]\n",
      "\n",
      "Fold 7: \n",
      "Coefficients:\t[-8.32934382e-01 -4.42782720e-01 -1.57917232e+00  4.76404835e-01\n",
      " -2.52125514e-02  3.21625417e+00  7.36937741e-01 -8.14999931e-01\n",
      "  1.64211284e+00 -9.54791048e-02 -5.30648181e-01  1.29631901e-01\n",
      "  2.89331156e-01  1.80256242e-03  1.44546051e-01  3.39866357e-02\n",
      "  1.46063037e+00 -5.94984030e-01  1.31619455e-01 -3.84074460e-03\n",
      "  9.08045708e-03 -1.41497476e-02  1.02441692e-03 -3.71491800e-02\n",
      " -3.72083939e-04  1.76610261e-02  3.89456916e-04  3.04881762e-03\n",
      " -2.75482704e-02 -1.08219315e-01  1.70068209e-02 -2.04426477e-05\n",
      " -1.11391535e-02 -4.74985836e-03  3.88758130e-04  4.49728909e-03\n",
      "  6.70312495e-04 -5.68140827e-03  2.59790636e-02 -3.27659052e-01\n",
      "  1.55271386e+00  1.11438769e-01  3.90723352e-03  6.45667127e-02\n",
      "  3.22853462e-02  6.04173288e-04 -5.05130940e-02 -5.24378180e-04\n",
      " -1.44996201e-02  4.76404835e-01 -1.04530602e+00 -2.02825666e+00\n",
      "  3.93207138e-02  3.31573338e-01 -2.08445245e-01  3.25901983e-04\n",
      "  8.13161165e-01  5.86425857e-03 -1.71341165e-01 -2.29010324e-01\n",
      "  3.05515210e+00 -4.17546023e-01  3.44412482e+00 -6.63395872e-01\n",
      " -2.18336871e-02 -1.69684255e+00  1.70549072e-03  1.27789585e+00\n",
      "  9.31737507e-01 -3.39912048e-02  2.53456000e-01 -1.59103594e-01\n",
      " -3.81209913e-03 -3.65767441e-01 -3.35900275e-04 -1.79072282e-01\n",
      "  6.67630266e-04  3.12565950e-03  7.39279241e-03 -2.66965883e-04\n",
      " -2.55888628e-03 -8.12015874e-04 -8.68804453e-03  3.11544344e-01\n",
      " -1.40664376e-01 -2.72215656e-03 -1.52821756e-01 -9.05407797e-03\n",
      "  9.14440736e-02 -1.51422955e-01  5.51974876e-03  3.96438682e-02\n",
      " -1.10080516e-03 -2.58701509e-02 -1.71799288e-05  5.61270600e-03\n",
      "  5.37042965e-05 -5.71929191e-04  7.07936970e-02 -1.49453785e-04\n",
      "  8.66487942e-03 -5.17652405e-05 -2.88821704e-04  1.85703009e-02]\n",
      "Test MSE:\t[11.69763038]\n",
      "\n",
      "Fold 8: \n",
      "Coefficients:\t[-2.89523512e-01 -1.37377929e-01 -1.90253842e+00  3.49823799e-01\n",
      "  3.78313697e-02  3.39293744e+00  1.03392668e+00 -7.05086161e-01\n",
      "  1.37014332e+00 -1.93334621e-03 -4.11228891e-01  5.28662290e-02\n",
      " -1.60055199e+00  4.53131280e-03  1.53872856e-01  3.62151903e-01\n",
      "  3.38116564e+00 -1.21436453e+00 -1.18711987e-02 -9.17957604e-03\n",
      " -2.19972317e-01  2.32432271e-01 -2.40788092e-02  2.91343841e-01\n",
      " -1.68830608e-04  7.98587773e-04  4.79529267e-04 -7.43389047e-05\n",
      " -6.91140285e-02 -7.62524032e-02 -1.48343104e-03  1.38578711e-04\n",
      " -9.64738253e-03 -7.15008190e-03  3.86997291e-04 -1.25127637e-03\n",
      "  4.28535182e-04 -6.88725658e-03  2.85827676e-02 -8.34712401e-01\n",
      "  9.60149043e-01  1.45177191e-01  3.49323360e-03  6.92811344e-02\n",
      " -1.77404628e-02  2.40927466e-04 -6.99501879e-02  1.85572716e-03\n",
      " -8.32058042e-03  3.49823799e-01 -1.19723508e+00 -4.48056539e+00\n",
      "  1.05231620e-01  1.94723335e+00 -1.41446371e+00  3.67592293e-02\n",
      "  1.03301544e+00  1.90355975e-02 -4.93074651e-01 -5.08786840e-02\n",
      "  2.97228464e+00 -4.16682413e-01  2.78299576e+00 -5.79783951e-01\n",
      " -5.36959711e-03 -1.65927960e+00  2.09869540e-03  1.81293527e+00\n",
      "  7.95208933e-01 -4.74726859e-02  3.52296704e-01 -9.90813538e-02\n",
      " -6.07894155e-03 -5.07016006e-01  9.80949462e-03 -3.40574991e-02\n",
      "  2.22029416e-04 -4.86278570e-03  7.71739344e-03 -3.23647213e-04\n",
      "  2.96641439e-03 -1.32609562e-03 -9.88623914e-03  2.65324197e-01\n",
      " -9.52545774e-02 -4.53546668e-03 -6.23530017e-02 -1.23003286e-02\n",
      "  1.45909152e-01 -1.19276554e-01  5.70286273e-03 -3.60120482e-02\n",
      "  1.21330086e-03 -1.48097732e-02 -1.99961104e-05  6.87465232e-03\n",
      " -1.71121361e-04 -6.25944557e-04  2.82971191e-02  4.03771057e-03\n",
      "  1.46011390e-02  8.11074425e-06  1.37618137e-04  2.69804002e-02]\n",
      "Test MSE:\t[16.85570605]\n",
      "\n",
      "Fold 9: \n",
      "Coefficients:\t[-1.54916337e-01 -3.42931202e-01 -1.82138094e+00  9.71707402e-01\n",
      "  1.27564231e-01  4.80035981e+00  9.47511492e-01 -1.43442739e+00\n",
      "  8.14102636e-01 -3.96365892e-02 -2.94329838e+00  5.05935030e-02\n",
      " -4.47688313e-01  2.68044714e-03  2.67587472e-01  5.38896330e-01\n",
      "  1.89528789e+00 -7.68831108e-01  8.79234079e-02 -2.27851429e-03\n",
      "  2.56708754e-02  5.18603427e-01 -4.28792770e-02  2.98116591e-01\n",
      " -1.58442183e-04  1.62194172e-02 -1.66980873e-04 -2.96581027e-03\n",
      " -2.11872047e-02 -3.18832195e-01  1.22469833e-02  7.83346734e-04\n",
      " -3.69689144e-03 -9.12234988e-03  4.18174016e-04 -2.29017478e-03\n",
      "  1.07361245e-03 -8.87797061e-03  2.11924662e-02 -6.47435414e-02\n",
      "  2.21743678e+00  1.67045858e-02  4.10813567e-03  5.90156370e-02\n",
      " -5.20621015e-02  1.64084098e-03 -8.36473414e-02  2.97034344e-03\n",
      " -2.96379856e-02  9.71707402e-01  1.55707413e-01 -4.60515779e+00\n",
      "  9.25134337e-02  3.06931534e+00 -1.01231170e-01 -1.56017546e-02\n",
      "  7.97185515e-01  1.45035250e-02 -3.18867570e-01  7.26404430e-02\n",
      "  4.14671905e+00 -5.53538052e-01  3.30344195e+00 -2.51762515e-01\n",
      " -8.22296061e-02 -6.32562865e-01  6.06248049e-03  1.25544902e+00\n",
      "  7.04338062e-01 -4.79576161e-02  9.35745487e-02 -3.00469209e-01\n",
      " -7.22769212e-04 -1.34197278e-01 -2.37533923e-03 -1.29871403e-01\n",
      "  3.53691370e-04 -8.19321046e-03  1.62838047e-02 -6.50070122e-04\n",
      "  9.56274445e-04 -8.21657301e-04 -6.93259744e-03  2.71295128e-01\n",
      " -9.03518465e-02 -4.57551347e-03 -3.84170905e-02 -8.30060925e-03\n",
      "  1.42615885e-01 -1.97630363e-01  1.03438478e-02  2.11537246e-02\n",
      "  9.58246470e-04 -4.05103924e-02 -4.45728254e-05  8.86878922e-03\n",
      " -1.87847873e-04  4.95084679e-04 -2.38491840e-02  6.07647496e-03\n",
      "  1.13623308e-02 -2.60266548e-05 -3.47018674e-04  2.21763892e-02]\n",
      "Test MSE:\t[10.31234194]\n",
      "\n",
      "Fold 10: \n",
      "Coefficients:\t[-4.61405712e-01 -2.00765853e-01 -1.50423886e+00 -1.39916982e-03\n",
      "  6.75698364e-02  2.29457611e+00  1.11118095e+00 -5.21206866e-01\n",
      "  8.40469551e-01 -1.05580128e-01 -9.98483440e-01  1.31684466e-01\n",
      "  6.51186466e-01  2.62554902e-03  1.91714601e-01  3.13376837e-01\n",
      "  1.47969969e+00 -4.90393605e-01  7.28921655e-02 -2.31700380e-03\n",
      " -9.58940307e-02  2.92214771e-01 -2.76792349e-02  2.84617988e-01\n",
      " -3.48712991e-04  2.26573220e-02  1.27959544e-04 -2.83207535e-03\n",
      " -6.52855115e-03 -2.10440738e-01  1.30898369e-02 -3.60932551e-04\n",
      " -9.73282265e-03 -4.26890859e-03  3.85410505e-04 -4.40664195e-03\n",
      "  7.00551180e-04 -3.76892033e-03  2.97769125e-02 -7.92455749e-02\n",
      "  3.39580317e-01  1.56893355e-01  3.76736895e-03  3.43738129e-02\n",
      "  2.00242077e-02  6.70907772e-04 -5.56895519e-02  8.46985579e-04\n",
      " -2.23823572e-02 -1.39916982e-03 -1.24017189e+00 -2.21533401e+00\n",
      " -6.21415360e-03 -2.34716723e-01 -7.21298117e-02 -5.76039713e-03\n",
      "  9.13816800e-01  1.07237936e-02 -7.50144436e-02 -6.63632631e-02\n",
      "  2.20596610e+00 -3.83084416e-01  2.42404890e+00 -1.25513769e+00\n",
      "  3.47842043e-02 -9.82857971e-01 -7.44417420e-03  9.56522197e-01\n",
      "  9.30792892e-01 -7.10672695e-02 -1.81179573e-01 -1.96177024e-01\n",
      "  1.17360097e-04 -1.46934517e-01  2.47558023e-03 -2.23786419e-01\n",
      "  2.50338257e-05  3.57085403e-03  1.12229331e-02 -4.97644597e-04\n",
      " -6.54081464e-03 -7.69502800e-04 -7.28692006e-03  3.34588305e-01\n",
      " -8.18425780e-02 -4.01788487e-03 -5.90797689e-02 -5.33126909e-03\n",
      "  5.71337860e-02 -1.34626333e-01  5.68473943e-03  4.97227266e-02\n",
      "  2.65686135e-04 -3.94062430e-02 -3.81416236e-05  7.29317729e-03\n",
      " -7.39853324e-05  3.12990832e-04  1.35941848e-02  7.29303872e-05\n",
      "  1.08475548e-02 -6.38730323e-05 -2.32834769e-04  1.39082969e-02]\n",
      "Test MSE:\t[20.26201193]\n",
      "\n",
      "\n",
      "AVERAGE TEST MSE: 16.67\n"
     ]
    }
   ],
   "source": [
    "# Ridge, 2-degree polynomial\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=0)\n",
    "ridge_reg.fit(X_poly, y)\n",
    "\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# performing linear regression with cross validation\n",
    "scores_ridge_2d = cross_validate(ridge_reg, X_poly, y, scoring='neg_mean_squared_error', cv=cv_custom_split, return_train_score=True, return_estimator=True)\n",
    "\n",
    "# displaying\n",
    "for (i, fold) in zip(range(1, len(scores_ridge_2d['estimator'])+1), scores_ridge_2d['estimator']):\n",
    "    print(f\"Fold {i}: \\nCoefficients:\\t{fold.coef_}\\nTest MSE:\\t{np.abs([scores_ridge_2d['test_score'][i-1]])}\\n\")  # coefficients and test error for each fold\n",
    "\n",
    "print(f\"\\nAVERAGE TEST MSE: {round(np.mean(-scores_ridge_2d['test_score']), 2)}\")  # average test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2976.589346142286, tolerance: 4.2716295415019765\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1865.6116120349227, tolerance: 3.4398062970297034\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2061.698260677521, tolerance: 3.121724158415842\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2292.522100729808, tolerance: 3.458104168316832\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2380.191007361241, tolerance: 3.0876655346534654\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2414.893830451693, tolerance: 3.4409350594059407\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2421.9152641336204, tolerance: 3.5215467995049505\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2210.661998303263, tolerance: 3.5861270594059405\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2290.9044336698435, tolerance: 3.6126867004950496\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2365.798956222934, tolerance: 3.376435128712871\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2034.8361683399662, tolerance: 3.422670910891089\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  9.14691795e-03 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  9.56571858e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -6.76820498e-03\n",
      " -0.00000000e+00 -8.10766522e-03  6.96874620e-04  0.00000000e+00\n",
      "  4.46758942e-04 -0.00000000e+00  7.99669817e-05 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  4.14442854e-02  1.42826056e-04\n",
      "  8.17189274e-03 -0.00000000e+00  3.22696443e-04 -2.01054626e-03\n",
      " -9.14911634e-04 -6.61728941e-03  1.81766485e-02 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  6.94015905e-03 -0.00000000e+00\n",
      " -4.85189283e-02  1.07472620e-03 -4.32340965e-02 -7.39903997e-05\n",
      " -2.81093201e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  3.78623304e-03\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -2.88558640e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  1.19183359e-02  1.16068330e-02  0.00000000e+00 -1.01447316e-01\n",
      " -1.00091647e-02 -0.00000000e+00  2.21553204e-02 -1.24033018e-02\n",
      "  9.38720152e-05  8.16384703e-03 -1.06605828e-04  2.06125263e-04\n",
      " -5.51695129e-03 -3.79938672e-04 -2.95110809e-03  2.54874264e-02\n",
      " -0.00000000e+00  1.69398387e-05  0.00000000e+00 -5.04899573e-03\n",
      " -0.00000000e+00 -5.21009420e-03  2.28198444e-03  3.41158541e-02\n",
      "  1.56157789e-05 -9.34477561e-03 -4.20916322e-05  4.04415185e-03\n",
      " -3.32214297e-05 -1.26464643e-03 -5.95511990e-03 -3.36608950e-03\n",
      " -0.00000000e+00  7.57950968e-06 -2.39862040e-04  3.16318852e-02]\n",
      "Test MSE:\t[25.99845669]\n",
      "\n",
      "Fold 2: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.25021336e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -2.13975412e-03\n",
      " -0.00000000e+00 -1.79878742e-02  3.79228839e-04  0.00000000e+00\n",
      " -1.87275245e-05  3.10415568e-03 -5.24997187e-05 -5.97882299e-03\n",
      "  0.00000000e+00  0.00000000e+00  1.93988494e-02 -1.91856433e-05\n",
      "  1.24244802e-03 -4.50642477e-03  5.20363102e-04  0.00000000e+00\n",
      " -5.70819545e-04 -3.65333349e-03  1.44729532e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  4.16107937e-03 -0.00000000e+00\n",
      " -4.71830544e-02  2.47482667e-04 -3.98306810e-02  1.07824054e-03\n",
      " -2.44134233e-02  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  4.60908585e-03\n",
      "  0.00000000e+00  3.29393847e-04  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -2.77976762e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  1.94571030e-01  1.78577934e-02  0.00000000e+00 -2.50844869e-02\n",
      " -8.50886706e-03 -0.00000000e+00  1.58951862e-02 -8.57003592e-02\n",
      "  2.26420502e-04  7.99156580e-03 -1.89668571e-03  1.73807192e-04\n",
      " -3.99994780e-03 -4.90013834e-04 -2.99635889e-03  5.39888515e-02\n",
      " -0.00000000e+00 -3.77559004e-03  0.00000000e+00 -2.56054299e-03\n",
      "  0.00000000e+00 -9.26148480e-03  2.16581402e-03  2.40115460e-02\n",
      "  8.12531460e-05 -1.67286381e-02 -6.35640137e-06  4.19350830e-03\n",
      " -5.71354589e-05 -6.51389129e-04 -3.71939131e-02 -7.07483189e-04\n",
      "  5.18179860e-03 -4.37460998e-05  2.40945763e-04  2.93970652e-02]\n",
      "Test MSE:\t[19.5751804]\n",
      "\n",
      "Fold 3: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  6.68396147e-03 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.84452271e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -2.23619853e-03\n",
      " -0.00000000e+00 -1.71108254e-02  3.99516266e-04  0.00000000e+00\n",
      "  4.68567553e-05  2.58151967e-03  7.96977420e-05 -2.37875899e-03\n",
      "  0.00000000e+00  0.00000000e+00  5.13952688e-02 -1.20871272e-04\n",
      "  2.19586928e-03 -4.24572207e-05  4.19599471e-04  8.62743755e-04\n",
      " -1.14945419e-03 -5.92065216e-03  1.75922566e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  4.75123322e-03 -0.00000000e+00\n",
      " -5.63583012e-02  9.63901572e-04 -4.54106071e-02  5.73106290e-04\n",
      " -2.59529800e-02  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.87697938e-03  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -3.18172743e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  1.42614379e-01  1.01670874e-02  0.00000000e+00 -9.09697282e-02\n",
      " -7.31380794e-03  0.00000000e+00  1.67504692e-02 -2.46587264e-02\n",
      "  4.39361837e-04  4.27325019e-03  7.44079455e-04  1.04001298e-04\n",
      " -3.48006736e-03 -3.45346873e-04 -5.41517156e-03  5.78313003e-02\n",
      " -0.00000000e+00 -2.35834304e-03  0.00000000e+00 -3.26906893e-03\n",
      "  0.00000000e+00 -7.58473670e-03  2.22716165e-03  3.59590150e-02\n",
      "  3.56755054e-04 -1.41658668e-02 -2.66615044e-05  3.75993050e-03\n",
      " -5.31951474e-05 -8.12362494e-04 -2.22649079e-02 -1.76266573e-03\n",
      "  8.59799615e-04 -2.94138665e-05  1.35720854e-04  3.07687880e-02]\n",
      "Test MSE:\t[14.30109184]\n",
      "\n",
      "Fold 4: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  2.43445021e-02 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  8.77399761e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -5.00896469e-03\n",
      " -0.00000000e+00 -7.73196713e-03  4.09351873e-04  0.00000000e+00\n",
      "  4.03687617e-04  8.12502569e-04  2.44662902e-04 -2.08564838e-03\n",
      "  0.00000000e+00 -0.00000000e+00  5.15252407e-02  7.50642321e-04\n",
      " -1.39976476e-03 -7.09563028e-03  4.08015670e-04  3.96198143e-03\n",
      " -1.34145953e-03 -0.00000000e+00  1.22305760e-02  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  3.29431710e-03 -0.00000000e+00\n",
      " -5.81042372e-02  5.68643929e-04 -2.86392470e-02  3.06272587e-04\n",
      " -1.38180680e-02  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.64145393e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -3.47303451e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  2.38996343e-01  7.64543881e-03  0.00000000e+00 -3.21017524e-02\n",
      " -1.14569390e-02  0.00000000e+00  1.41020043e-02 -0.00000000e+00\n",
      "  1.55872680e-04  4.77116326e-04 -2.95986744e-04  2.24969484e-04\n",
      " -0.00000000e+00 -4.49725804e-04 -2.83449101e-03  7.46616248e-02\n",
      " -1.07880945e-03 -3.81928276e-03  0.00000000e+00 -2.00307323e-03\n",
      "  0.00000000e+00 -1.87200940e-03  1.64953673e-03  3.87412151e-02\n",
      " -1.11475305e-04 -9.31755403e-03 -2.19010554e-05  4.27037476e-03\n",
      " -2.16208412e-05 -1.70096518e-03 -4.92122091e-02 -4.91637353e-04\n",
      " -5.56146322e-03 -3.08669027e-05 -3.89167670e-04  3.21808754e-02]\n",
      "Test MSE:\t[13.39292356]\n",
      "\n",
      "Fold 5: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  1.20343842e-02 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  1.65389386e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -6.32365271e-03\n",
      " -0.00000000e+00 -1.57818695e-02  8.35207464e-04  0.00000000e+00\n",
      "  4.96927834e-04 -1.96673319e-04  4.84440560e-04  1.91978786e-04\n",
      "  0.00000000e+00  0.00000000e+00  2.82013343e-02  8.00418263e-04\n",
      " -0.00000000e+00 -7.41029447e-03  4.83718892e-04  3.55699263e-03\n",
      " -1.00871860e-03 -5.69480910e-03  1.34255829e-02  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  6.50960048e-03 -0.00000000e+00\n",
      " -5.67239933e-02  8.30650971e-04 -4.05981556e-02  4.38372655e-04\n",
      " -2.72607766e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  5.57502759e-03\n",
      " -0.00000000e+00 -4.72258756e-04 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -2.48845127e-02 -0.00000000e+00 -1.26377147e-02  0.00000000e+00\n",
      "  1.44191027e-01  1.10268041e-02  0.00000000e+00 -7.67783128e-02\n",
      " -1.08683752e-02  0.00000000e+00  1.91814912e-02 -2.16959661e-02\n",
      " -5.94506075e-05  0.00000000e+00 -1.09287307e-03  2.35534249e-04\n",
      " -1.78137081e-03 -4.40865439e-04 -2.72337117e-03  7.07863521e-02\n",
      " -2.41324139e-03 -1.83431129e-03  0.00000000e+00 -3.58627323e-03\n",
      " -0.00000000e+00 -7.63996678e-03  1.91072735e-03  4.56002293e-02\n",
      "  3.85209780e-04 -1.01285042e-02 -2.63476531e-05  4.26340218e-03\n",
      " -4.57157525e-05 -1.44009067e-03 -2.86612102e-02 -2.17192956e-03\n",
      " -5.65722419e-07 -4.76334476e-06 -1.54718551e-04  3.37879093e-02]\n",
      "Test MSE:\t[12.16730281]\n",
      "\n",
      "Fold 6: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  1.52173454e-02 -0.00000000e+00  1.44522491e-02\n",
      " -0.00000000e+00  3.26197956e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -4.23343144e-03\n",
      " -0.00000000e+00 -1.40101140e-02  5.64791620e-04  0.00000000e+00\n",
      "  5.13040896e-04 -0.00000000e+00  1.71374075e-04 -2.49952170e-03\n",
      "  0.00000000e+00  0.00000000e+00  4.23664189e-02  3.15998340e-04\n",
      " -0.00000000e+00 -6.14940296e-03  4.52569562e-04  8.73438674e-04\n",
      " -9.77082553e-04 -5.08461241e-03  1.24781069e-02  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  6.36348167e-03 -0.00000000e+00\n",
      " -7.50355682e-02  6.83090799e-04 -4.66000788e-02  1.27901038e-03\n",
      " -2.97970003e-02  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.53248121e-03\n",
      "  0.00000000e+00  5.42148504e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -3.92429326e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  1.19862226e-01  3.05777486e-02  0.00000000e+00 -1.05679263e-01\n",
      " -1.02021396e-02  0.00000000e+00  1.66808509e-02 -3.89866963e-02\n",
      " -4.06949924e-04  0.00000000e+00  1.84791066e-03  1.69531992e-04\n",
      " -4.69114783e-03 -5.48614857e-04 -1.17922515e-03  4.62394035e-02\n",
      " -0.00000000e+00 -2.80579316e-03  0.00000000e+00 -2.28763534e-03\n",
      " -0.00000000e+00 -8.86451619e-04  2.17382228e-03  4.80928187e-02\n",
      "  3.26912495e-04 -2.39677135e-02 -2.96949942e-05  5.09158144e-03\n",
      " -6.18501096e-05 -9.12078000e-04 -3.20376257e-02 -1.93738683e-03\n",
      " -0.00000000e+00 -2.56719654e-05 -2.02834813e-04  3.27595645e-02]\n",
      "Test MSE:\t[12.7928621]\n",
      "\n",
      "Fold 7: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  3.86769729e-03 -0.00000000e+00  1.13141590e-02\n",
      "  0.00000000e+00  2.66228415e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -3.78252434e-03\n",
      " -0.00000000e+00 -1.94570996e-02  4.99185393e-04  0.00000000e+00\n",
      "  4.64869215e-06  6.08659659e-03  1.20494696e-04  7.70109679e-04\n",
      "  0.00000000e+00 -0.00000000e+00  3.66630033e-02  4.45030236e-04\n",
      " -1.16169842e-03 -9.32385112e-03  5.03998266e-04  2.38967371e-03\n",
      " -9.03073787e-04 -8.55753263e-03  1.95097382e-02 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.99190449e-03 -0.00000000e+00\n",
      " -4.90518214e-02  3.85341888e-04 -4.13838834e-02  8.75256417e-04\n",
      " -2.96787363e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  1.28316132e-02\n",
      "  0.00000000e+00 -5.90135668e-03 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -2.66454059e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  2.35432681e-01  1.50963820e-02  0.00000000e+00 -1.46236770e-02\n",
      " -1.15488202e-02 -0.00000000e+00  1.56439445e-02 -8.27799270e-03\n",
      "  3.96962010e-04  3.59809167e-03 -1.19606480e-03  2.35412965e-04\n",
      " -6.29708395e-04 -5.63638639e-04 -7.53046032e-03  7.39679003e-02\n",
      " -0.00000000e+00 -3.62033486e-03  0.00000000e+00 -2.46262859e-03\n",
      "  0.00000000e+00 -7.18255841e-03  1.98492014e-03  3.54276192e-02\n",
      " -4.22679742e-04 -1.26607790e-02 -2.46717041e-05  4.66561263e-03\n",
      " -5.58013665e-06 -1.31962995e-03 -3.39274788e-02 -1.81973362e-03\n",
      "  0.00000000e+00 -4.68680236e-05  2.93177282e-04  3.92767728e-02]\n",
      "Test MSE:\t[17.91668663]\n",
      "\n",
      "Fold 8: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  3.72425460e-02 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  3.24440100e-03  0.00000000e+00  3.78414641e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -3.87374053e-03\n",
      " -0.00000000e+00 -2.12232498e-02  7.87010860e-04  0.00000000e+00\n",
      "  1.99382321e-04 -6.18033352e-03  4.97559680e-04 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  4.55160405e-02  2.41476132e-04\n",
      " -5.68627765e-03 -5.92079588e-03  5.72458621e-04  5.84849726e-04\n",
      " -1.14418916e-03 -3.31247006e-03  6.97131283e-03 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.78884206e-03 -0.00000000e+00\n",
      " -7.62067275e-02  9.07345257e-04 -3.54631826e-02  7.00923570e-04\n",
      " -1.91887202e-02  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.51647636e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -2.08464031e-02 -0.00000000e+00 -1.05981278e-02  0.00000000e+00\n",
      "  0.00000000e+00  3.06378486e-02  0.00000000e+00 -9.61921540e-02\n",
      " -1.22729100e-02 -0.00000000e+00  2.26519229e-02 -4.18705178e-02\n",
      "  1.18106322e-05  5.09439519e-03 -9.66843708e-04  1.76205081e-04\n",
      "  0.00000000e+00 -7.93628830e-04 -4.46418863e-03  1.45715671e-01\n",
      " -0.00000000e+00 -3.04031246e-03  0.00000000e+00 -4.74777814e-03\n",
      "  0.00000000e+00 -2.79303982e-03  2.45093127e-03  4.95898355e-02\n",
      "  1.58974212e-04 -8.56975679e-03 -4.27267497e-05  3.97047565e-03\n",
      " -3.89392282e-05 -1.34061846e-03 -2.66420637e-02 -2.22566029e-03\n",
      " -5.16752855e-03  6.52951691e-07  2.25691787e-04  3.48305580e-02]\n",
      "Test MSE:\t[15.73902181]\n",
      "\n",
      "Fold 9: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  1.34809918e-02 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  2.52918448e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -4.36480571e-03\n",
      " -0.00000000e+00 -9.41590534e-03  3.55759420e-04 -0.00000000e+00\n",
      "  4.00196809e-05  2.33193598e-03  3.11682319e-04 -2.93354777e-03\n",
      "  0.00000000e+00 -0.00000000e+00  3.74558437e-02  3.69683289e-04\n",
      "  0.00000000e+00 -4.78412634e-03  4.13002463e-04  2.02123044e-03\n",
      " -9.71679013e-04 -4.43208510e-03  1.23391737e-02 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.16574796e-03 -0.00000000e+00\n",
      " -4.80123345e-02  9.34920985e-04 -4.09917535e-02  4.50962204e-04\n",
      " -2.20957167e-02  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -7.67429104e-04\n",
      "  0.00000000e+00  2.16360638e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -3.16670029e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  2.76100626e-01  1.57850821e-02  0.00000000e+00 -8.72051215e-02\n",
      " -1.07241434e-02  0.00000000e+00  1.35734099e-02 -0.00000000e+00\n",
      "  1.87784452e-04  5.82381488e-03 -5.62969629e-05  2.09865466e-04\n",
      " -0.00000000e+00 -6.09848886e-04 -5.28003982e-03  7.09515881e-02\n",
      " -0.00000000e+00 -1.14477763e-03  0.00000000e+00 -4.79120156e-03\n",
      "  0.00000000e+00 -4.87487945e-03  1.80345443e-03  4.76831308e-02\n",
      " -2.92633247e-04 -4.68842552e-03 -2.42119278e-05  3.82709781e-03\n",
      " -2.59108486e-05 -1.62664337e-03 -5.14920909e-02  2.45345338e-04\n",
      " -7.87139920e-03 -1.48437863e-05  1.21946159e-04  3.76982168e-02]\n",
      "Test MSE:\t[12.66469391]\n",
      "\n",
      "Fold 10: \n",
      "Coefficients:\t[ 0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  7.54558920e-03 -0.00000000e+00  1.57268793e-02\n",
      "  0.00000000e+00  3.15081950e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -2.77854500e-03\n",
      " -0.00000000e+00 -2.35334644e-02  3.40417198e-04  0.00000000e+00\n",
      " -7.53723395e-05  1.13089129e-02  2.80898857e-04 -6.17916896e-04\n",
      "  0.00000000e+00  0.00000000e+00  3.58772699e-02  4.02078453e-04\n",
      " -0.00000000e+00 -9.54806620e-03  3.83425609e-04 -0.00000000e+00\n",
      " -8.01982405e-04 -4.88657774e-03  1.01296689e-02 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  4.88467997e-03 -0.00000000e+00\n",
      " -6.10161052e-02  7.80901082e-04 -3.36630601e-02  6.69452923e-04\n",
      " -2.49131636e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  1.43504755e-02\n",
      "  0.00000000e+00 -8.20680482e-03  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -2.59200693e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  2.81541967e-03  3.06670689e-02  0.00000000e+00 -2.66349926e-02\n",
      " -9.15054110e-03 -0.00000000e+00  2.06098581e-02 -9.18117522e-02\n",
      " -4.14428454e-04  5.84494101e-03 -5.72373945e-04  6.80986337e-05\n",
      " -0.00000000e+00 -6.84765114e-04 -1.09741946e-03  1.41430394e-01\n",
      " -3.43633690e-03 -4.25119355e-03  0.00000000e+00 -4.44452499e-03\n",
      "  0.00000000e+00 -6.26909055e-03  1.95038816e-03  4.58485279e-02\n",
      " -6.08290641e-04 -8.57856879e-03 -1.88686648e-05  4.31157972e-03\n",
      " -2.61887144e-05 -1.38716410e-03 -3.94046397e-02 -1.50951825e-03\n",
      "  0.00000000e+00 -7.17718954e-05  6.19940044e-04  3.08986562e-02]\n",
      "Test MSE:\t[22.52934346]\n",
      "\n",
      "\n",
      "AVERAGE TEST MSE: 16.71\n"
     ]
    }
   ],
   "source": [
    "# Lasso, 2-degree polynomial\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "lasso_reg = Lasso(alpha=1)\n",
    "lasso_reg.fit(X_poly, y)\n",
    "\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# performing linear regression with cross validation\n",
    "scores_lasso_2d = cross_validate(lasso_reg, X_poly, y, scoring='neg_mean_squared_error', cv=cv_custom_split, return_train_score=True, return_estimator=True)\n",
    "\n",
    "# displaying\n",
    "for (i, fold) in zip(range(1, len(scores_lasso_2d['estimator'])+1), scores_lasso_2d['estimator']):\n",
    "    print(f\"Fold {i}: \\nCoefficients:\\t{fold.coef_}\\nTest MSE:\\t{np.abs([scores_lasso_2d['test_score'][i-1]])}\\n\")  # coefficients and test error for each fold\n",
    "\n",
    "print(f\"\\nAVERAGE TEST MSE: {round(np.mean(-scores_lasso_2d['test_score']), 2)}\")  # average test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Set Mean Squared Errors:\n",
      "\tRidge Linear: 21.45\n",
      "\tLasso Linear: 26.21\n",
      "\tRidge Poly: 5.96\n",
      "\tLasso Poly: 9.5\n",
      "\n",
      "\n",
      "TEST Set Mean Squared Errors:\n",
      "\tRidge Linear: 26.0\n",
      "\tLasso Linear: 30.77\n",
      "\tRidge Poly: 16.67\n",
      "\tLasso Poly: 16.71\n"
     ]
    }
   ],
   "source": [
    "# Displaying Sumamry (2.4)\n",
    "\n",
    "print('TRAIN Set Mean Squared Errors:')\n",
    "print(f\"\\tRidge Linear: {round(np.mean(-scores_ridge_linear['train_score']), 2)}\")\n",
    "print(f\"\\tLasso Linear: {round(np.mean(-scores_lasso_linear['train_score']), 2)}\")\n",
    "print(f\"\\tRidge Poly: {round(np.mean(-scores_ridge_2d['train_score']), 2)}\")\n",
    "print(f\"\\tLasso Poly: {round(np.mean(-scores_lasso_2d['train_score']), 2)}\\n\")\n",
    "\n",
    "print('\\nTEST Set Mean Squared Errors:')\n",
    "print(f\"\\tRidge Linear: {round(np.mean(-scores_ridge_linear['test_score']), 2)}\")\n",
    "print(f\"\\tLasso Linear: {round(np.mean(-scores_lasso_linear['test_score']), 2)}\")\n",
    "print(f\"\\tRidge Poly: {round(np.mean(-scores_ridge_2d['test_score']), 2)}\")\n",
    "print(f\"\\tLasso Poly: {round(np.mean(-scores_lasso_2d['test_score']), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings (Question 2.4):\n",
    "\n",
    "- Lasso brings some coefficients to 0, eliminating the consideration of those variables, Ridge does not\n",
    "- 2nd degree increases the number of variables, and Lasso brings even a greater percentage of those to zero\n",
    "- With Lasso, when going from linear to 2nd degree features the number of variables considered increases, meaning the polynomial features are relevant in this case\n",
    "- Using 2nd degree polynomial transformations **reduces de test error substantially**, meaning they are enough for the model to overfit and meaning we should use them\n",
    "- Lasso's feature selection makes the model a simpler one with exclusively relevant variables\n",
    "- **Best option: Lasso with 2nd degree polynomials**: although the train error is higher and the test error slightly higher too, there is value in having a simpler model - one with much less variables considered. I believe the decrease in variables considered is worth the increase in the MSEs. **Anyway, Ridge 2nd degree would also be a good option.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (30 points) Cancer detection\n",
    "\n",
    "Given a dataset with features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, that describe characteristics of the cell nuclei present in the image. Let's try to predict whether the pateints is diagnosed as Malignant (M) or Benign (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.1 (10 points) \n",
    "Use logistic regression to train the dataset through cross-validation, report the score on train and test set, respectively. Explain what do you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ShuffleSplit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8512c4b9c276>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# making sure each fold is 20% test set and 80% training set (from the documentation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcv_custom_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# performing linear regression of the 2-degree polynomials with cross validation while keeping the train and test errors at each fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ShuffleSplit' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# initializing the model\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# performing linear regression of the 2-degree polynomials with cross validation while keeping the train and test errors at each fold\n",
    "scores = cross_validate(log_reg, X, y, cv=cv_custom_split, return_train_score=True)  # default score is returned - accuracy\n",
    "\n",
    "# creating a dataframe for proper error visualization (each fold and average)\n",
    "    # each fold\n",
    "dic_scores = {'Train set Accuracy': np.round(scores['train_score'], 2),\n",
    "        'Test set Accuracy': np.round(scores['test_score'], 2)\n",
    "       }\n",
    "\n",
    "df_scores = pd.DataFrame(dic_scores, index = [i for i in range(1, 11)])\n",
    "\n",
    "    # average\n",
    "avg_train_mse = round(np.mean(df_scores['Train set Accuracy']), 2)\n",
    "avg_test_mse = round(np.mean(df_scores['Test set Accuracy']), 2)\n",
    "\n",
    "avg_df = pd.DataFrame([[avg_train_mse, avg_test_mse]], columns=['Train set Accuracy', 'Test set Accuracy'], index=['Average'])\n",
    "df_scores.append(avg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings (Question 3.1)\n",
    "\n",
    "- Train and test scores are identical. At first, the model looks quite efficent\n",
    "- However, accuracy is not a good indicator for a logistic regression anyway\n",
    "- The most important metric is the number of false negatives, even though we should also keep an eye on the false positives\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2 (10 points) \n",
    "By default, sklearn's logistic regression uses the L2 regularization. Now use the logistic regression without any regularzation to perform cross validation, report what do you find on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\xwork\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train set Score</th>\n",
       "      <th>Test set Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train set Score  Test set Score\n",
       "1                   0.99            0.96\n",
       "2                   0.99            0.97\n",
       "3                   1.00            0.97\n",
       "4                   1.00            0.94\n",
       "5                   1.00            0.95\n",
       "6                   0.99            0.97\n",
       "7                   1.00            0.96\n",
       "8                   1.00            0.92\n",
       "9                   1.00            0.97\n",
       "10                  0.99            0.96\n",
       "Average             1.00            0.96"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as 3.1 but with a large C parameter on logisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", C=10**100, random_state=0, penalty='l1')\n",
    "\n",
    "# making sure each fold is 20% test set and 80% training set (from the documentation)\n",
    "cv_custom_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating a dictionary with the results\n",
    "scores = cross_validate(log_reg, X, y, cv=cv_custom_split, return_train_score=True) # default score is returned\n",
    "\n",
    "# creating a dataframe for proper error visualization (each fold and average)\n",
    "    # each fold\n",
    "dic_scores = {'Train set Score': np.round(scores['train_score'], 2),\n",
    "        'Test set Score': np.round(scores['test_score'], 2)\n",
    "       }\n",
    "\n",
    "df_scores = pd.DataFrame(dic_scores, index = [i for i in range(1, 11)])\n",
    "\n",
    "    # average\n",
    "avg_train_mse = round(np.mean(df_scores['Train set Score']), 2)\n",
    "avg_test_mse = round(np.mean(df_scores['Test set Score']), 2)\n",
    "\n",
    "avg_df = pd.DataFrame([[avg_train_mse, avg_test_mse]], columns=['Train set Score', 'Test set Score'], index=['Average'])\n",
    "df_scores.append(avg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings (Question 3.2):\n",
    "\n",
    "- As expected, not including any regularization causes better accuracy on the training set\n",
    "- Test set accuracy increased slightly by not using regularization. This confirms that accuracy is not the right metric\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.3 (10 points) \n",
    "Check how many Benign and Malignant cases in the dataset. What might be the problem if we use the default score of the logistic regression cross-validation. Now adjust the class weight of M and L to make them balanced and retrain the model again, report what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 569. Benign: 357, Malign: 212\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset\n",
    "total_count = len(y)\n",
    "benign_count = sum(y)\n",
    "malign_count = total_count - benign_count\n",
    "print(f\"Total: {total_count}. Benign: {benign_count}, Malign: {malign_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top right is the worst that can happen, since this is the numbers of undiagnosed maligniant cancer cases. Bottom left is also bad (diagnosed but not real malignant cancer cases)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Balanced dataset, confusion matrix')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unbalanced dataset\n",
    "log_reg = LogisticRegression(solver = 'liblinear', random_state = 0)\n",
    "y_pred_unbalanced = cross_val_predict(log_reg, X, y, cv = 10)\n",
    "confusion_matrix_unbalanced = confusion_matrix(y, y_pred_unbalanced, labels=[0, 1])\n",
    "\n",
    "# Balanced dataset\n",
    "log_reg = LogisticRegression(solver = 'liblinear', random_state = 0, class_weight = \"balanced\")\n",
    "y_pred_balanced = cross_val_predict(log_reg, X, y, cv = 10)\n",
    "confusion_matrix_balanced = confusion_matrix(y, y_pred_balanced, labels=[0, 1])\n",
    "\n",
    "# Displaying\n",
    "print('Top right is the worst that can happen, since this is the numbers of undiagnosed maligniant cancer cases. Bottom left is also bad (diagnosed but not real malignant cancer cases)')\n",
    "ConfusionMatrixDisplay(confusion_matrix_unbalanced).plot()\n",
    "plt.title('Unbalanced dataset, confusion matrix')\n",
    "ConfusionMatrixDisplay(confusion_matrix_balanced).plot()\n",
    "plt.title('Balanced dataset, confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Scores - ability of not to label as positive a sample that is negative:\n",
      "    Unbalanced: 0.95\n",
      "    Balanced: 0.96\n",
      "\n",
      "Recall Scores - ability to find all the positive samples:\n",
      "        Unbalanced: 0.97\n",
      "        Balanced: 0.96\n",
      "\n",
      "F1 Scores - weighted average of the precision and recall:\n",
      "        Unbalanced: 0.96\n",
      "        Balanced: 0.96\n",
      "\n",
      "Area Under the Curve Scores - compares the true positive and false positive rates:\n",
      "        Unbalanced: 0.94\n",
      "        Balanced: 0.95\n",
      "\n",
      "Key changes in the confusion matrix, when balancing the dataset:\n",
      "        False negatives avoided (most important): 5\n",
      "        False positives created: 4\n"
     ]
    }
   ],
   "source": [
    "# Proper evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# precision score\n",
    "print(f\"\"\"Precision Scores - ability of not to label as positive a sample that is negative:\n",
    "    Unbalanced: {round(precision_score(y, y_pred_unbalanced),2)}\n",
    "    Balanced: {round(precision_score(y, y_pred_balanced),2)}\\n\"\"\")\n",
    "\n",
    "# recall score\n",
    "print(f\"\"\"Recall Scores - ability to find all the positive samples:\n",
    "        Unbalanced: {round(recall_score(y, y_pred_unbalanced),2)}\n",
    "        Balanced: {round(recall_score(y, y_pred_balanced),2)}\\n\"\"\")\n",
    "\n",
    "# F1\n",
    "print(f\"\"\"F1 Scores - weighted average of the precision and recall:\n",
    "        Unbalanced: {round(f1_score(y, y_pred_unbalanced),2)}\n",
    "        Balanced: {round(f1_score(y, y_pred_balanced),2)}\\n\"\"\")\n",
    "\n",
    "# Area under the curve score\n",
    "print(f\"\"\"Area Under the Curve Scores - compares the true positive and false positive rates:\n",
    "        Unbalanced: {round(roc_auc_score(y, y_pred_unbalanced),2)}\n",
    "        Balanced: {round(roc_auc_score(y, y_pred_balanced),2)}\\n\"\"\")\n",
    "\n",
    "# Confusion Matrix Summary\n",
    "print(f\"\"\"Key changes in the confusion matrix, when balancing the dataset:\n",
    "        False negatives avoided (most important): {confusion_matrix_unbalanced[0][1] - confusion_matrix_balanced[0][1]}\n",
    "        False positives created: {confusion_matrix_balanced[1][0] - confusion_matrix_unbalanced[1][0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings (Question 3.3):\n",
    "\n",
    "- The problem with using accuracy as the measure with an **unbalaced** dataset is that the skewness of the data might induce the model into predicting one class very well but not the other \n",
    "- The problem with using accuracy for this case as the measure even with a balanced dataset is that the false negatives matter much more than anything else, and therefore a more comprehensive method is needed, such as a confusion matrix\n",
    "- Balancing the dataset resulted in false negatives going from 18 to 13 and, most importantly, stop outweighing the false positives (which are not as bad), although these increased from 11 to 15\n",
    "- In general, balancing the dataset resulted in a more efficient model, and one that predicts better the most important cases too.\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
